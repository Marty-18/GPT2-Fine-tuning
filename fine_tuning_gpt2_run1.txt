DEVICE: cuda
Commencing training!
Arguments: {'data': 'austen_dataset.pkl', 'batch_size': 8, 'max_input_length': 100, 'train_size': 0.9, 'max_output_length': 200, 'top_k': 50, 'top_p': 0.95, 'model': 'gpt2', 'warmup_steps': 100.0, 'sample_every': 1170, 'epochs': 4, 'lr': 0.0005, 'eps': 1e-08, 'check_dir': 'Checkpoints', 'restore_file': None, 'save_interval': 1, 'load_checkpoint': False, 'output_dir': 'GPT2_fine_tuned_Austen'}
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 137kB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 16.0MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 43.4MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 17.8MB/s]
config.json: 100% 665/665 [00:00<00:00, 2.69MB/s]
Total length of data loaded 41594
Number of samples for training = 37434
Number of samples for validation = 4160
model.safetensors: 100% 548M/548M [00:06<00:00, 84.4MB/s]
generation_config.json: 100% 124/124 [00:00<00:00, 666kB/s]
Total training steps 18720.
Loaded a base model from HuggingFace.
Beginning epoch 1 of 4
| Epoch 001:  11% 500/4680 [01:59<16:27,  4.23it/s] Step 500 of 4680. Loss: 1.004655361175537. Time: 0:02:00
| Epoch 001:  21% 1000/4680 [03:58<14:39,  4.18it/s] Step 1000 of 4680. Loss: 1.241495966911316. Time: 0:03:59
| Epoch 001:  25% 1170/4680 [04:39<13:57,  4.19it/s]Generating example output... at step 1170 of epoch 1.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput: representMiss Crawford's complexion seemed to feel the effects of the shock.


| Epoch 001:  32% 1500/4680 [05:59<12:30,  4.24it/s] Step 1500 of 4680. Loss: 1.29137122631073. Time: 0:06:00
| Epoch 001:  43% 2000/4680 [07:58<10:38,  4.20it/s] Step 2000 of 4680. Loss: 1.0816760063171387. Time: 0:07:58
| Epoch 001:  50% 2340/4680 [09:18<09:12,  4.24it/s]Generating example output... at step 2340 of epoch 1.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput: acing"Oh, I will tell you I had a delightful evening," said Bingley; "for my father's sake, I should be ashamed of him."


| Epoch 001:  53% 2500/4680 [09:57<08:35,  4.23it/s] Step 2500 of 4680. Loss: 0.8568471074104309. Time: 0:09:57
| Epoch 001:  64% 3000/4680 [11:55<06:36,  4.24it/s] Step 3000 of 4680. Loss: 0.9609245657920837. Time: 0:11:56
| Epoch 001:  75% 3500/4680 [13:54<04:40,  4.20it/s] Step 3500 of 4680. Loss: 1.1858458518981934. Time: 0:13:55
| Epoch 001:  75% 3510/4680 [13:56<04:37,  4.21it/s]Generating example output... at step 3510 of epoch 1.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput:  pluginsI am the only one who would ever wish to go to her home, but I was the one only who did not give up a day's exercise at her house, and whose not to be able to go to her if she did not come there.


| Epoch 001:  85% 4000/4680 [15:53<02:41,  4.22it/s] Step 4000 of 4680. Loss: 0.8732534646987915. Time: 0:15:54
| Epoch 001:  96% 4500/4680 [17:52<00:43,  4.16it/s] Step 4500 of 4680. Loss: 1.02057945728302. Time: 0:17:52
Average Training Loss: 1.1242456858866234. Epoch time: 0:18:35

Validation loss: 1.0037916347384452. Validation Time: 0:00:36

Saving model after 1 epochs.
------------------------------
Beginning epoch 2 of 4
| Epoch 002:  11% 500/4680 [01:58<16:29,  4.23it/s] Step 500 of 4680. Loss: 0.9240959882736206. Time: 0:01:59
| Epoch 002:  21% 1000/4680 [03:57<14:32,  4.22it/s] Step 1000 of 4680. Loss: 1.1286317110061646. Time: 0:03:57
| Epoch 002:  25% 1170/4680 [04:37<13:47,  4.24it/s]Generating example output... at step 1170 of epoch 2.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput:  stimulationHe called as follows:


| Epoch 002:  32% 1500/4680 [05:55<12:27,  4.25it/s] Step 1500 of 4680. Loss: 1.140601396560669. Time: 0:05:56
| Epoch 002:  43% 2000/4680 [07:53<10:31,  4.25it/s] Step 2000 of 4680. Loss: 0.9792004227638245. Time: 0:07:54
| Epoch 002:  50% 2340/4680 [09:14<09:13,  4.23it/s]Generating example output... at step 2340 of epoch 2.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput: qualityâ€œAnd she really will?


| Epoch 002:  53% 2500/4680 [09:52<08:42,  4.17it/s] Step 2500 of 4680. Loss: 0.7965995073318481. Time: 0:09:52
| Epoch 002:  64% 3000/4680 [11:50<06:42,  4.17it/s] Step 3000 of 4680. Loss: 0.8537458777427673. Time: 0:11:51
| Epoch 002:  75% 3500/4680 [13:48<04:39,  4.22it/s] Step 3500 of 4680. Loss: 1.0632655620574951. Time: 0:13:49
| Epoch 002:  75% 3510/4680 [13:51<04:37,  4.21it/s]Generating example output... at step 3510 of epoch 2.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput: ___I had not known that Frank Churchill would ever meet with his friend without being struck with it; or, perhaps, when the time came, to be struck by it.


| Epoch 002:  85% 4000/4680 [15:47<02:40,  4.25it/s] Step 4000 of 4680. Loss: 0.7894192934036255. Time: 0:15:48
| Epoch 002:  96% 4500/4680 [17:46<00:42,  4.24it/s] Step 4500 of 4680. Loss: 0.8702741861343384. Time: 0:17:46
Average Training Loss: 0.9490477437965381. Epoch time: 0:18:29

Validation loss: 1.002126215169063. Validation Time: 0:00:36

Saving model after 2 epochs.
------------------------------
Beginning epoch 3 of 4
| Epoch 003:  11% 500/4680 [01:58<16:28,  4.23it/s] Step 500 of 4680. Loss: 0.7303813695907593. Time: 0:01:59
| Epoch 003:  21% 1000/4680 [03:57<14:35,  4.20it/s] Step 1000 of 4680. Loss: 0.8596465587615967. Time: 0:03:57
| Epoch 003:  25% 1170/4680 [04:37<13:50,  4.23it/s]Generating example output... at step 1170 of epoch 3.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput:  HealingHe could never speak the unreserved truth of Mrs. Elton's engagement to Mr. Darcy; she would never even acknowledge the truth of it, but his own actions proved very civil, and he was never silent without the pleasure of receiving a recommendation from a friend, a man in general, to join his father's family.


| Epoch 003:  32% 1500/4680 [05:56<12:28,  4.25it/s] Step 1500 of 4680. Loss: 0.9489438533782959. Time: 0:05:57
| Epoch 003:  43% 2000/4680 [07:54<10:32,  4.24it/s] Step 2000 of 4680. Loss: 0.8701545000076294. Time: 0:07:55
| Epoch 003:  50% 2340/4680 [09:15<09:12,  4.24it/s]Generating example output... at step 2340 of epoch 3.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput: ,"How well you puzzle Mr. Woodhouse!


| Epoch 003:  53% 2500/4680 [09:53<08:33,  4.24it/s] Step 2500 of 4680. Loss: 0.6377496719360352. Time: 0:09:53
| Epoch 003:  64% 3000/4680 [11:51<06:38,  4.21it/s] Step 3000 of 4680. Loss: 0.7337097525596619. Time: 0:11:52
| Epoch 003:  75% 3500/4680 [13:49<04:39,  4.23it/s] Step 3500 of 4680. Loss: 0.8611606359481812. Time: 0:13:50
| Epoch 003:  75% 3510/4680 [13:52<04:36,  4.23it/s]Generating example output... at step 3510 of epoch 3.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput:  MaMr. Price had not looked at her; he had hardly even inquired; he was not quite satisfied with his words: he had just reached the last page.


| Epoch 003:  85% 4000/4680 [15:48<02:40,  4.24it/s] Step 4000 of 4680. Loss: 0.6361445188522339. Time: 0:15:49
| Epoch 003:  96% 4500/4680 [17:47<00:42,  4.22it/s] Step 4500 of 4680. Loss: 0.7117860317230225. Time: 0:17:47
Average Training Loss: 0.7818396305108172. Epoch time: 0:18:30

Validation loss: 1.0332298618669693. Validation Time: 0:00:36

Saving model after 3 epochs.
------------------------------
Beginning epoch 4 of 4
| Epoch 004:  11% 500/4680 [01:58<16:39,  4.18it/s] Step 500 of 4680. Loss: 0.44308948516845703. Time: 0:01:59
| Epoch 004:  21% 1000/4680 [03:57<14:31,  4.22it/s] Step 1000 of 4680. Loss: 0.4692552089691162. Time: 0:03:58
| Epoch 004:  25% 1170/4680 [04:37<13:59,  4.18it/s]Generating example output... at step 1170 of epoch 4.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput:  borrowHe could never give her a moment's uneasiness.


| Epoch 004:  32% 1500/4680 [05:56<12:35,  4.21it/s] Step 1500 of 4680. Loss: 0.7569674253463745. Time: 0:05:57
| Epoch 004:  43% 2000/4680 [07:55<10:34,  4.22it/s] Step 2000 of 4680. Loss: 0.6838899850845337. Time: 0:07:55
| Epoch 004:  50% 2340/4680 [09:15<09:20,  4.18it/s]Generating example output... at step 2340 of epoch 4.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput: usuallyThe only thing that remained to be done was to go out of doors, so Jane was off; but at last the doors were closed, and Mr. Wickham, unattended by the housekeeper, stepped forward to her, and taking leave, seemed to see Elizabeth leaning back in bed, with anxious parade of anxious solicitude on a wet afternoon, the apprehension of his coming back, and her being obliged to dance with him.


| Epoch 004:  53% 2500/4680 [09:54<08:42,  4.17it/s] Step 2500 of 4680. Loss: 0.5416467189788818. Time: 0:09:55
| Epoch 004:  64% 3000/4680 [11:53<06:39,  4.21it/s] Step 3000 of 4680. Loss: 0.5997403860092163. Time: 0:11:54
| Epoch 004:  75% 3500/4680 [13:52<04:42,  4.17it/s] Step 3500 of 4680. Loss: 0.6885492205619812. Time: 0:13:52
| Epoch 004:  75% 3510/4680 [13:54<04:38,  4.20it/s]Generating example output... at step 3510 of epoch 4.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Example ouput:  wrapperMr. Woodhouse, whose eyes had filled with the vain solicitude for her, and whose cheeks were also turned pale, soon disappeared.


| Epoch 004:  85% 4000/4680 [15:51<02:42,  4.18it/s] Step 4000 of 4680. Loss: 0.5528249144554138. Time: 0:15:52
| Epoch 004:  96% 4500/4680 [17:50<00:43,  4.17it/s] Step 4500 of 4680. Loss: 0.5854613184928894. Time: 0:17:51
Average Training Loss: 0.598571954985969. Epoch time: 0:18:33

Validation loss: 1.082652555234157. Validation Time: 0:00:36

Saving model after 4 epochs.
------------------------------
Total training took 1:17:18
[{'epoch': 1, 'Training Loss': 1.1242456858866234, 'Valid. Loss': 1.0037916347384452, 'Training Time': '0:18:35', 'Validation Time': '0:00:36', 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7bbdf0702290>}, {'epoch': 2, 'Training Loss': 0.9490477437965381, 'Valid. Loss': 1.002126215169063, 'Training Time': '0:18:29', 'Validation Time': '0:00:36', 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7bbdf0702290>}, {'epoch': 3, 'Training Loss': 0.7818396305108172, 'Valid. Loss': 1.0332298618669693, 'Training Time': '0:18:30', 'Validation Time': '0:00:36', 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7bbdf0702290>}, {'epoch': 4, 'Training Loss': 0.598571954985969, 'Valid. Loss': 1.082652555234157, 'Training Time': '0:18:33', 'Validation Time': '0:00:36', 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7bbdf0702290>}]
FINISHED TRAINING SUCCESSFULLY!